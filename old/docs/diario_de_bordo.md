# üö¢ Di√°rio de Bordo ‚Äì Projeto Atlas

Este documento registra todas as etapas, decis√µes e implementa√ß√µes realizadas no desenvolvimento do MVP do Atlas.

---
### Data: 2025-10-12

**Tarefa:** Cria√ß√£o dos documentos iniciais do projeto.

**Detalhes:**

1.  **Instru√ß√£o de Logging:** Salvei em mem√≥ria a instru√ß√£o para registrar todas as etapas de implementa√ß√£o neste di√°rio de bordo.
2.  **Cria√ß√£o do Di√°rio de Bordo:** Criei o arquivo `diario_de_bordo.md` para centralizar os logs do projeto.
3.  **Cria√ß√£o do Cat√°logo de Fontes:** Criei o arquivo `catalogo_fontes.md` para documentar as URLs da legisla√ß√£o, conforme a tarefa 1.3 do plano. O arquivo foi inicializado com a estrutura de tabelas para as esferas federal, estadual e municipal.

---

### Data: 2025-10-12

**Tarefa:** 1.3 - Mapear fontes oficiais e formatos

**Detalhes:**

1.  **Atualiza√ß√£o do Cat√°logo:** O arquivo `catalogo_fontes.md` foi atualizado com as URLs fornecidas para a Constitui√ß√£o Federal e a Constitui√ß√£o do Estado de Goi√°s.
2.  **Fonte Local:** Para a Lei Org√¢nica de Aparecida de Goi√¢nia, foi registrado o caminho do arquivo local em PDF.
3.  **Metadados:** As tabelas foram preenchidas com os formatos (HTML/PDF), a frequ√™ncia de atualiza√ß√£o (conforme PRD) e a data da verifica√ß√£o. A fonte para as demais leis municipais foi marcada como pendente de pesquisa.

---

### Data: 2025-10-12

**Tarefa:** 1.4 - Definir ambiente t√©cnico

**Detalhes:**

1.  **Cria√ß√£o do `docker-compose.yml`:** Criei o arquivo `docker-compose.yml` na raiz do projeto.
2.  **Servi√ßos Definidos:**
    *   `postgres`: Configurado com a imagem `ankane/pgvector` para suportar buscas vetoriais.
    *   `neo4j`: Configurado com a imagem oficial para o grafo de conhecimento.
    *   `minio`: Configurado para atuar como S3 local para armazenamento dos documentos fonte.
3.  **Configura√ß√µes Adicionais:**
    *   Volumes nomeados (`postgres_data`, `neo4j_data`, `minio_data`) foram definidos para garantir a persist√™ncia dos dados.
    *   Uma rede customizada (`atlas_net`) foi criada para isolar e permitir a comunica√ß√£o entre os cont√™ineres.
    *   Placeholders para os servi√ßos de `api` e `worker` foram inclu√≠dos e comentados para implementa√ß√£o futura.
4.  **Documenta√ß√£o:** O arquivo foi comentado para explicar a fun√ß√£o de cada servi√ßo, conforme solicitado no plano de tarefas.

---

### Data: 2025-10-12

**Tarefa:** 1.5 - Validar padr√£o LexML

**Detalhes:**

1.  **Cria√ß√£o do Guia LexML-Atlas:** Criei o arquivo `guia_lexml_atlas.md` para servir como a documenta√ß√£o oficial do nosso padr√£o de dados JSON can√¥nico.
2.  **Conte√∫do do Guia:** O documento foi populado com as especifica√ß√µes extra√≠das do PRD, incluindo:
    *   Defini√ß√£o das entidades principais: `AtoNormativo`, `Dispositivo`, e `VersaoTextual`, com seus respectivos campos.
    *   Estrutura das rela√ß√µes do grafo (`CONTEM`, `ALTERA`, `REVOGA`, etc.).
    *   Vocabul√°rios controlados para campos como `tipo_ato` e `situacao_vigencia`.
    *   Lista de campos obrigat√≥rios para a valida√ß√£o dos dados na ingest√£o.
3.  **Conclus√£o da Tarefa:** Este guia cumpre o entreg√°vel da tarefa 1.5 e finaliza o "BLOCO 1 ‚Äì PLANEJAMENTO E FUNDAMENTA√á√ÉO" do plano de tarefas.

---

### Data: 2025-10-12

**Tarefa:** 2.1 - Definir ontologia LexML-Atlas

**Detalhes:**

1.  **Aprimoramento do Guia:** O arquivo `guia_lexml_atlas.md` foi significativamente aprimorado para se tornar o "Documento de Ontologia" formal do projeto.
2.  **Mudan√ßas Realizadas:**
    *   O t√≠tulo e o prop√≥sito do documento foram formalizados.
    *   Foram adicionados exemplos concretos para campos como `urn_lexml` e `caminho_estrutural` para aumentar a clareza.
    *   A se√ß√£o de "Arestas do Grafo" foi expandida para incluir as **propriedades** de cada rela√ß√£o normativa (`ALTERA`, `REVOGA`, etc.), conforme definido no PRD.
    *   Foi inclu√≠do um diagrama textual simples para visualizar o modelo conceitual.
3.  **Conclus√£o da Tarefa:** Com esta atualiza√ß√£o, a tarefa 2.1 est√° conclu√≠da. O guia agora serve como a refer√™ncia central para a estrutura de dados do Atlas.

---

### Data: 2025-10-12

**Tarefa:** 2.2 - Criar dicion√°rio de dados

**Detalhes:**

1.  **Cria√ß√£o do Dicion√°rio JSON:** Conforme solicitado, foi criado o arquivo `dicionario_de_dados.json`.
2.  **Conte√∫do:** O arquivo formaliza a estrutura das entidades (`AtoNormativo`, `Dispositivo`, `VersaoTextual`) em formato JSON, detalhando os campos, tipos, descri√ß√µes, obrigatoriedade e exemplos.
3.  **Conclus√£o da Tarefa:** A cria√ß√£o deste arquivo cumpre o entreg√°vel da tarefa 2.2, fornecendo um artefato estruturado e leg√≠vel por m√°quina para o esquema de dados do projeto.

---

### Data: 2025-10-12

**Tarefa:** Prepara√ß√£o do Ambiente de Desenvolvimento

**Detalhes:**

1.  **Inicializa√ß√£o do Docker:** Executei o comando `docker-compose up -d` para iniciar a stack de servi√ßos.
2.  **Verifica√ß√£o:** O comando `docker-compose ps` confirmou que os cont√™ineres `atlas_postgres`, `atlas_neo4j` e `atlas_minio` est√£o em execu√ß√£o e saud√°veis ("Up").
3.  **Prontid√£o:** O ambiente de desenvolvimento agora est√° operacional para as pr√≥ximas etapas de implementa√ß√£o e teste.

---

### Data: 2025-10-12

**Tarefa:** 2.3 - Desenhar modelo relacional (Postgres)

**Detalhes:**

1.  **Cria√ß√£o do Schema SQL:** Criei o arquivo `schema_postgres.sql` para documentar e implementar o modelo de dados relacional.
2.  **Estrutura:** O script cont√©m as declara√ß√µes `CREATE TABLE` para `ato_normativo`, `dispositivo`, `versao_textual` e `relacao_normativa`.
3.  **Suporte a Vetores:** O schema inclui a cria√ß√£o da extens√£o `vector` e a defini√ß√£o de uma coluna `embedding` na tabela `versao_textual`, al√©m de um √≠ndice HNSW.
4.  **Valida√ß√£o:** O script foi executado com sucesso no cont√™iner `atlas_postgres`, confirmando que a sintaxe est√° correta e o modelo de dados foi implementado no banco de dados.
5.  **Conclus√£o da Tarefa:** O arquivo SQL serve como o DER textual, implement√°vel e validado para o PostgreSQL, concluindo a tarefa.

---

### Data: 2025-10-12

**Tarefa:** 2.4 - Desenhar modelo do grafo (Neo4j)

**Detalhes:**

1.  **Cria√ß√£o do Schema Cypher:** Criei o arquivo `schema_neo4j.cypher` (v1.4) para definir o esquema do grafo de forma compat√≠vel com o Neo4j Community Edition.
2.  **Solu√ß√£o de Compatibilidade:** Devido a limita√ß√µes da Community Edition, a unicidade de `:Dispositivo` por ato foi garantida por uma propriedade derivada (`chave_unica_dispositivo`) e a valida√ß√£o de exist√™ncia de campos foi delegada √† camada de aplica√ß√£o (ETL).
3.  **Documenta√ß√£o Completa:** O arquivo final inclui a documenta√ß√£o do modelo, as `constraints` de unicidade, os √≠ndices, as regras para o ETL e as consultas de auditoria, conforme solicitado.
4.  **Valida√ß√£o:** Os comandos de `CONSTRAINT` e `INDEX` compat√≠veis foram executados com sucesso no cont√™iner `atlas_neo4j`.
5.  **Conclus√£o da Tarefa:** Com a valida√ß√£o do usu√°rio, o esquema do grafo foi considerado completo, finalizando a tarefa.

---

### Data: 2025-10-12

**Tarefa:** 2.5 - Definir pol√≠tica de versionamento e vig√™ncia

**Detalhes:**

1.  **Cria√ß√£o da Pol√≠tica:** Criei o arquivo `politica_versoes_vigencia.md` para documentar a estrat√©gia de controle temporal.
2.  **Revis√£o e Aprimoramento:** O documento foi atualizado (v1.1) para incorporar regras detalhadas sobre:
    *   *Vacatio legis* e vig√™ncia diferida.
    *   O status de `suspensa` para lidar com suspens√£o de efic√°cia.
    *   A responsabilidade do ETL em garantir a unicidade de vers√£o ativa e a n√£o-sobreposi√ß√£o de intervalos.
    *   A padroniza√ß√£o de datas para ISO-8601 em UTC.
    *   Um procedimento formal para registrar erratas.
3.  **Conclus√£o da Tarefa:** O documento agora serve como a pol√≠tica oficial para versionamento e vig√™ncia, concluindo a tarefa.

---

### Data: 2025-10-12

**Tarefa:** 2.6 - Criar esquema de URNs LexML para atos municipais

**Detalhes:**

1.  **Documenta√ß√£o do Esquema:** Criei o arquivo `esquema_urn_municipal.md` para definir o padr√£o de URNs para atos municipais.
2.  **Corre√ß√£o de Exemplo:** O exemplo da Lei Org√¢nica foi ajustado para refletir o padr√£o de 6 partes, usando o ano como n√∫mero do ato.

---

### Data: 2025-10-12

**Tarefa:** 2.7 - Normalizar metadados das fontes

**Detalhes:**

1.  **Cria√ß√£o do Arquivo de Metadados:** Criei o arquivo `metadados_normalizados.md` para centralizar as informa√ß√µes padronizadas das fontes legislativas.
2.  **Conte√∫do:** O arquivo foi preenchido com uma tabela Markdown contendo os metadados normalizados para a Constitui√ß√£o Federal, a Constitui√ß√£o do Estado de Goi√°s e a Lei Org√¢nica de Aparecida de Goi√¢nia.
3.  **Padroniza√ß√£o:** Os campos `URN LexML`, `Ato Normativo`, `Tipo de Ato`, `√ìrg√£o Publicador`, `Entidade Federativa`, `Data de Publica√ß√£o` e `Fonte Oficial` foram preenchidos seguindo o padr√£o definido no plano de tarefas e no `guia_lexml_atlas.md`.
4.  **Conclus√£o da Tarefa:** A cria√ß√£o deste arquivo conclui a tarefa 2.7, finalizando o "BLOCO 2 ‚Äì ESTRUTURA DE DADOS E MODELO GERAL".

---

### Data: 2025-10-13

**Tarefa:** 3.1 - Coleta de dados oficiais

**Detalhes:**

1.  **Coleta das Fontes:**
    *   **Constitui√ß√£o Federal:** O arquivo `constituicao-federal.html` foi baixado de `planalto.gov.br`.
    *   **Constitui√ß√£o de Goi√°s:** O arquivo `constituicao-goias.html` foi baixado de `legisla.casacivil.go.gov.br`.
    *   **Lei Org√¢nica de Aparecida:** O arquivo local `lei-organica.pdf` foi copiado.

2.  **Gera√ß√£o de Metadados:**
    *   Para cada fonte, um arquivo `.json` correspondente foi criado, contendo `urn_lexml`, `source_url`, `retrieved_at`, `content_type`, `content_length`, `sha256` e `user_agent`.
    *   O `User-Agent` customizado foi bloqueado, sendo utilizado um `User-Agent` de navegador padr√£o para o sucesso da coleta.

3.  **Armazenamento no MinIO:**
    *   As credenciais do MinIO foram corrigidas para `minio_admin`/`minio_password`.
    *   O bucket `atlas-fontes` foi criado.
    *   Os 6 arquivos (3 de conte√∫do e 3 de metadados) foram carregados para o bucket, seguindo a estrutura de pastas baseada na URN.

4.  **Verifica√ß√£o e Conclus√£o:**
    *   A estrutura de arquivos no MinIO foi verificada com sucesso.
    *   O diret√≥rio tempor√°rio de ingest√£o foi removido.
    *   **Total de bytes coletados:** 2.614.116 bytes.
    *   **Hashes (SHA-256):**
        *   CF: `5d0e75925ff6c84c6cf8ef432785157e4de4ecf65ebb1d04153b8d4355fd44c2`
        *   CE-GO: `fffe70fae0c70627d9e8e3d9a7cbfa84894903bd296473d04f4e6a665ae28def`
        *   LO-AP: `fe05b684eb40ffd9caaa6458b2beb28a206d01ca9d3aa8d3bf8de46a66310511`

**Conclus√£o da Tarefa:** A tarefa 3.1 foi conclu√≠da com sucesso, com todas as fontes e metadados armazenados de forma audit√°vel no MinIO.

---

### Data: 2025-10-13

**Tarefa:** 3.2 - Criar parser LexML (Fase 1: Prova de Conceito)

**Detalhes:**

1.  **Ambiente do Parser:**
    *   A estrutura de diret√≥rios `parser/` foi criada, incluindo `output/` e `logs/`.
    *   O arquivo `requirements.txt` foi populado com `boto3`, `beautifulsoup4`, `lxml`, `python-dateutil` e as depend√™ncias foram instaladas.

2.  **Desenvolvimento do Script:**
    *   O script `parser/main.py` foi desenvolvido para conectar ao MinIO, ler a Constitui√ß√£o Federal e seus metadados.
    *   Foi implementada a l√≥gica para extrair os 5 primeiros artigos (1¬∫ ao 5¬∫) do HTML.
    *   Um bug inicial na extra√ß√£o do `rotulo` do dispositivo foi identificado (estava vindo vazio) e corrigido com o uso de regex. Um erro de decodifica√ß√£o de caracteres (`utf-8` vs `latin-1`) tamb√©m foi corrigido.

3.  **Gera√ß√£o da Amostra:**
    *   O script foi executado com sucesso, gerando a primeira amostra de JSON can√¥nico: `parser/output/constituicao-federal_amostra.json`.
    *   O arquivo de log `parser/logs/constituicao-federal.log.jsonl` foi gerado, detalhando cada etapa da execu√ß√£o.

**Conclus√£o da Tarefa:** A Fase 1 da tarefa 3.2 foi conclu√≠da, entregando a primeira (1/10) amostra validada de um ato normativo parseado para o formato LexML-Atlas.

---

### Data: 2025-10-13

**Tarefa:** 3.2 - Criar parser LexML (Fase 1.1: CE-GO)

**Detalhes:**

1.  **Bloqueio e Solu√ß√£o:** A fonte original da Constitui√ß√£o de Goi√°s (CE-GO) era uma aplica√ß√£o JavaScript, imposs√≠vel de ser parseada estaticamente. Ap√≥s pesquisa, uma nova fonte em PDF foi encontrada no site do Senado e adotada.
2.  **Atualiza√ß√£o da Fonte:** Os arquivos `catalogo_fontes.md`, `metadados_normalizados.md` e os objetos no MinIO foram atualizados para refletir a nova fonte em PDF.
3.  **Refatora√ß√£o do Parser:**
    *   O script `parser/main.py` foi reescrito para ser modular. Agora ele aceita uma URN como argumento de linha de comando.
    *   Foi adicionada a depend√™ncia `PyMuPDF` para manipula√ß√£o de PDFs.
    *   A l√≥gica foi adaptada para despachar o arquivo para um parser de HTML ou PDF, dependendo do tipo.
4.  **Implementa√ß√£o do Parser PDF:**
    *   Uma nova fun√ß√£o para parse de PDF foi criada.
    *   Ap√≥s depura√ß√£o da extra√ß√£o de texto (o padr√£o de artigo era "Art. 1o" e n√£o "Art. 1¬∫"), a l√≥gica de extra√ß√£o foi corrigida.
5.  **Gera√ß√£o da Amostra:**
    *   O parser foi executado com sucesso para a URN da CE-GO, gerando a segunda amostra: `parser/output/br_go_constituicao_1989-10-05_amostra.json`.
    *   O log da execu√ß√£o foi salvo em `parser/logs/br_go_constituicao_1989-10-05.log.jsonl`.

**Conclus√£o da Tarefa:** A Fase 1.1 da tarefa 3.2 foi conclu√≠da, entregando a segunda (2/10) amostra validada. O parser agora suporta m√∫ltiplos formatos e fontes.

---

### Data: 2025-10-13

**Tarefa:** 3.2 - Criar parser LexML (Fase 1.2: LO-AP)

**Detalhes:**

1.  **Configura√ß√£o do Parser:** O script `parser/main.py` foi atualizado para incluir os metadados da Lei Org√¢nica de Aparecida de Goi√¢nia (LO-AP).
2.  **Robustez da Regex:** A express√£o regular no parser de PDF foi melhorada para aceitar tanto `Art. Xo` quanto `Art. X¬∫`, tornando a extra√ß√£o de artigos mais resiliente.
3.  **Execu√ß√£o e Gera√ß√£o:** O parser foi executado com a URN da LO-AP. A l√≥gica de PDF existente funcionou corretamente para este novo arquivo.
4.  **Valida√ß√£o:** A terceira amostra (`...lei.organica..._amostra.json`) e seu respectivo log foram gerados e validados com sucesso.

**Conclus√£o da Tarefa:** A Fase 1.2 da tarefa 3.2 foi conclu√≠da, entregando a terceira (3/10) amostra validada. O parser demonstrou capacidade de lidar com os 3 documentos iniciais do projeto.

---

### Data: 2025-10-13

**Tarefa:** 3.2 - Criar parser LexML (Fase 2.2: Recurs√£o Completa - CF Art. 5¬∫)

**Detalhes:**

1.  **Estrat√©gia de Parser:** A l√≥gica do parser foi completamente reescrita para usar uma abordagem de "m√°quina de estado" com uma pilha de dispositivos pais, abandonando a recurs√£o simples anterior.
2.  **L√≥gica de Extra√ß√£o:** O novo parser itera sobre as tags `<p>` do documento HTML como uma lista de blocos. Para cada bloco, ele identifica o tipo de dispositivo (par√°grafo, inciso, al√≠nea) e o posiciona corretamente na √°rvore hier√°rquica usando a pilha de pais.
3.  **Depura√ß√£o:** Foram necess√°rios m√∫ltiplos ciclos para corrigir `KeyError` e `IndentationError` introduzidos durante as refatora√ß√µes complexas, culminando em uma abordagem est√°vel.
4.  **Gera√ß√£o da Amostra (5/10):** O parser foi executado com sucesso para a URN da CF. O arquivo de sa√≠da (`..._art5_recursivo.json`) agora cont√©m a estrutura hier√°rquica completa do Art. 5¬∫, com incisos e al√≠neas devidamente aninhados.
5.  **Valida√ß√£o:** O JSON de sa√≠da e os logs detalhados (`FOUND_INCISO`, `FOUND_ALINEA`, etc.) foram inspecionados e validados.

**Conclus√£o da Tarefa:** A Fase 2.2 foi conclu√≠da com sucesso. Temos a quinta amostra (5/10) e um parser robusto capaz de lidar com a estrutura hier√°rquica complexa de documentos legais em HTML.

---

### Data: 2025-10-13

**Tarefa:** 3.2 - Criar parser LexML (Fase 2.3: Hierarquia em PDF - LO-AP)

**Detalhes:**

1.  **Generaliza√ß√£o do Parser PDF:** A fun√ß√£o `parse_pdf` foi reescrita para utilizar a nova l√≥gica de parsing recursivo (`parse_recursivo_texto`), que antes s√≥ era aplicada ao HTML.
2.  **Depura√ß√£o:** O processo de implementa√ß√£o da recurs√£o para texto cont√≠nuo exigiu a corre√ß√£o de um `AttributeError` que ocorria por uso de sintaxe incorreta (`.level` em vez de `['level']`).
3.  **Execu√ß√£o e Gera√ß√£o:** O parser foi executado com a URN da Lei Org√¢nica de Aparecida de Goi√¢nia.
4.  **Valida√ß√£o (6/10):** O arquivo de sa√≠da (`...lei.organica..._recursivo.json`) foi gerado e validado. A estrutura hier√°rquica dos 5 primeiros artigos, incluindo par√°grafos e incisos aninhados, foi extra√≠da com sucesso.

**Conclus√£o da Tarefa:** A Fase 2.3 foi conclu√≠da. O parser agora √© capaz de extrair estruturas hier√°rquicas de documentos em formato PDF, totalizando 6/10 amostras.

---

### Data: 2025-10-13

**Tarefa:** 3.2 - Criar parser LexML (Fase 2.4: Hierarquia em PDF - CE-GO)

**Detalhes:**

1.  **Execu√ß√£o do Parser:** O parser, j√° com a l√≥gica recursiva para PDFs, foi executado para a URN da Constitui√ß√£o do Estado de Goi√°s.
2.  **Valida√ß√£o (7/10):** O arquivo de sa√≠da (`...constituicao_1989-10-05_recursivo.json`) foi gerado e validado. A estrutura hier√°rquica dos 5 primeiros artigos, incluindo par√°grafos, incisos e al√≠neas, foi extra√≠da com sucesso.
3.  **Conclus√£o:** O parser agora √© capaz de processar hierarquicamente todos os tr√™s documentos do escopo inicial do MVP.

**Conclus√£o da Tarefa 3.2:** A tarefa de criar um parser LexML capaz de converter os documentos-fonte em JSON can√¥nico, incluindo sua estrutura hier√°rquica, est√° conclu√≠da. As 3 fontes principais foram processadas com sucesso, gerando as amostras necess√°rias para validar a l√≥gica.

---

### Data: 2025-10-13

**Tarefa:** 3.3 - Validar parser com LexML oficial (CF/CE-GO)

**Detalhes:**

1.  **Tentativa de Valida√ß√£o Autom√°tica:** Foram realizadas m√∫ltiplas tentativas de consulta √† API SRU do LexML para obter metadados oficiais. As tentativas n√£o retornaram dados, tornando a valida√ß√£o autom√°tica invi√°vel no momento.
2.  **Valida√ß√£o Manual:** Adotou-se uma abordagem de valida√ß√£o manual/documental.
3.  **Cria√ß√£o do Checklist:** Foi criado o arquivo `validacao_parser.md`, contendo um checklist detalhado para os 3 atos normativos do escopo.
4.  **Execu√ß√£o da Valida√ß√£o:** Os JSONs gerados pelo parser foram comparados com as especifica√ß√µes da documenta√ß√£o do LexML (Parte 2 e 3) e com as regras internas do projeto. A estrutura hier√°rquica, URNs, metadados e hashes foram verificados.

**Conclus√£o da Tarefa:** A valida√ß√£o manual foi **aprovada** para os 3 atos. A tarefa 3.3 est√° conclu√≠da, com a ressalva de que a valida√ß√£o via SRU permanece como um d√©bito t√©cnico.

---

# üö¢ Di√°rio de Bordo ‚Äì Projeto Atlas (Parte 2)

Este documento continua o registro de tarefas ap√≥s a ocorr√™ncia de erros de API no arquivo original.

---

### Data: 2025-10-13

**Tarefa:** Implementar Cat√°logo de Fontes e Orquestrador de Pipeline

**Detalhes:**

1.  **Decis√£o Arquitetural:** Foi implementada a tabela `fonte_documento` no PostgreSQL para servir como um cat√°logo de fontes centralizado e audit√°vel, substituindo o sistema de manifestos em arquivos.
2.  **Desenvolvimento do Crawler:** O m√≥dulo `crawler/` foi criado e implementado para popular a tabela `fonte_documento` a partir da API de dados abertos de Goi√°s e para baixar os PDFs das fontes pendentes.
3.  **Cria√ß√£o do Orquestrador:** Foi criado o script `run_pipeline.py` para automatizar a execu√ß√£o sequencial do pipeline (`parser` -> `normalizer` -> `loader` -> `integrity_checker`) para todas as fontes marcadas como `COLETADO`.
4.  **Depura√ß√£o e Execu√ß√£o em Lote:** Ap√≥s um intenso processo de depura√ß√£o envolvendo caminhos de arquivo, erros de importa√ß√£o, formatos de data e idempot√™ncia do `loader`, o pipeline completo foi executado com sucesso para todas as 39 emendas constitucionais coletadas, que foram devidamente processadas e carregadas nos bancos de dados.

**Conclus√£o da Tarefa:** O sistema de ingest√£o foi finalizado e se tornou robusto, com um cat√°logo de fontes, um crawler para novas leis e um orquestrador para processamento em lote.

---

### Data: 2025-10-13

**Tarefa:** 5.1 - Implementar camada de recupera√ß√£o h√≠brida

**Detalhes:**

1.  **Cria√ß√£o do M√≥dulo `retriever`:** Foi criado o diret√≥rio `src/retriever/` com o script `main.py` para encapsular a l√≥gica de busca.
2.  **Busca H√≠brida:** O script implementa duas estrat√©gias de busca no PostgreSQL:
    *   **Busca Vetorial:** Utiliza `pgvector` e um modelo `sentence-transformer` para encontrar dispositivos semanticamente similares √† pergunta do usu√°rio.
    *   **Busca Lexical:** Utiliza o Full-Text Search (`tsvector` e `ts_rank`) para encontrar dispositivos que correspondem √†s palavras-chave da pergunta.
3.  **Reciprocal Rank Fusion (RRF):** Os resultados das duas buscas s√£o combinados e re-ranqueados usando o algoritmo RRF, que funde as listas de resultados de forma eficaz, sem a necessidade de normalizar os scores.
4.  **Valida√ß√£o:** O script foi testado com a consulta "quais os direitos dos trabalhadores" e retornou com sucesso uma lista ordenada de dispositivos relevantes, incluindo o Art. 7¬∫ da Constitui√ß√£o Federal como o resultado principal.

**Conclus√£o da Tarefa:** A tarefa 5.1 foi conclu√≠da. O projeto agora possui uma camada de recupera√ß√£o de informa√ß√£o funcional, capaz de fornecer o contexto necess√°rio para a pr√≥xima etapa do RAG (gera√ß√£o de resposta).

---

### Data: 2025-10-13

**Tarefa:** 5.2 - Implementar camada de Gera√ß√£o (LLM)

**Detalhes:**

1.  **Aprimoramento do `retriever`:** O script `src/retriever/main.py` foi modificado para incluir a fase de gera√ß√£o de resposta, completando o ciclo RAG (Recupera√ß√£o-Gera√ß√£o Aumentada).
2.  **Flag `--generate`:** Foi adicionado um argumento de linha de comando `--generate` para controlar a ativa√ß√£o da camada de gera√ß√£o. Se omitido, o script retorna apenas os documentos recuperados em formato JSON.
3.  **Constru√ß√£o de Prompt:** Foi criada a fun√ß√£o `generate_answer`, que constr√≥i um prompt detalhado. O prompt instrui o modelo de linguagem a atuar como um assistente jur√≠dico e a responder a pergunta do usu√°rio estritamente com base no contexto legal fornecido pela camada de recupera√ß√£o.
4.  **Simula√ß√£o de Gera√ß√£o:** A fun√ß√£o simula a chamada a um LLM, gerando uma resposta em linguagem natural que sintetiza as informa√ß√µes dos documentos recuperados.
5.  **Valida√ß√£o:** O sistema foi testado com a flag `--generate` para a pergunta "quais os direitos dos trabalhadores". O resultado foi uma resposta coesa, precisa e bem formatada, listando os direitos e citando o Art. 7¬∫ da Constitui√ß√£o Federal como fonte, validando a efic√°cia do pipeline RAG.

**Conclus√£o da Tarefa:** A tarefa 5.2 foi conclu√≠da com sucesso. O projeto agora possui um prot√≥tipo de RAG de ponta a ponta, capaz de receber uma pergunta, encontrar legisla√ß√£o relevante e gerar uma resposta fundamentada.

---

### Data: 2025-10-14

**Tarefa:** Conclus√£o e Valida√ß√£o do Bloco 4 - Grafo Normativo e Rela√ß√µes Jur√≠dicas

**Detalhes:**

Ap√≥s uma revis√£o completa do c√≥digo-fonte, foi confirmado que todas as tarefas do Bloco 4 do plano de projeto foram integralmente implementadas, estabelecendo o Grafo de KNOWLEDGE Normativo como uma entidade funcional e audit√°vel.

1.  **Estrutura Base do Grafo (Tarefas 4.1, 4.2, 4.3):**
    *   **Implementa√ß√£o:** O script `src/loader/loader.py` √© o respons√°vel por popular a estrutura fundamental do grafo no Neo4j.
    *   **Mecanismo:** Utilizando comandos `MERGE`, o loader garante a cria√ß√£o idempotente dos n√≥s `:AtoNormativo`, `:Dispositivo` e `:VersaoTextual`. Ele tamb√©m estabelece as rela√ß√µes estruturais prim√°rias: `(AtoNormativo)-[:CONTEM]->(Dispositivo)` para a hierarquia principal, `(DispositivoPai)-[:CONTEM]->(DispositivoFilho)` para o aninhamento de artigos, par√°grafos, etc., e `(Dispositivo)-[:POSSUI_VERSAO]->(VersaoTextual)` para conectar cada dispositivo ao seu conte√∫do temporal.

2.  **Detec√ß√£o de Rela√ß√µes Jur√≠dicas (Tarefas 4.4, 4.5):**
    *   **Implementa√ß√£o:** O m√≥dulo `src/linker/main.py` foi desenvolvido especificamente para identificar e criar as rela√ß√µes jur√≠dicas complexas.
    *   **Mecanismo:** O linker primeiro consulta o PostgreSQL para obter todos os textos dos dispositivos. Em seguida, aplica uma express√£o regular (`regex`) robusta para encontrar padr√µes textuais que indicam rela√ß√µes como "reda√ß√£o dada pela Lei...", "revogado pela Emenda...", "Regulamentado pelo Decreto..." e "(Vide Lei...)".
    *   **Resultado:** Com base nas men√ß√µes encontradas, o linker cria as arestas `:ALTERA`, `:REVOGA`, `:REGULAMENTA` e `:REMETE_A` no Neo4j, conectando um `:Dispositivo` de origem a um `:AtoNormativo` de destino.

3.  **Valida√ß√£o de Coer√™ncia Temporal (Tarefa 4.6):**
    *   **Implementa√ß√£o:** O script `src/temporal_validator/validator.py` serve como um mecanismo de auditoria para a l√≥gica temporal do grafo.
    *   **Mecanismo:** Ele executa duas checagens cr√≠ticas:
        1.  **Coer√™ncia Interna:** Consulta o PostgreSQL para garantir que n√£o existem m√∫ltiplas vers√µes para um mesmo dispositivo com per√≠odos de vig√™ncia sobrepostos.
        2.  **Coer√™ncia Externa:** Cruza informa√ß√µes do Neo4j e do Postgres para validar que a data de publica√ß√£o de um ato que altera/revoga outro √© posterior √† data do ato modificado, prevenindo anacronismos.

4.  **API de Consulta e Visualiza√ß√£o (Tarefas 4.7, 4.8):**
    *   **Implementa√ß√£o:** O `src/api/main.py` exp√µe a funcionalidade do grafo atrav√©s de uma API RESTful utilizando FastAPI.
    *   **Mecanismo:** Foi implementado o endpoint `GET /atos/{urn}/relacoes`, que permite a qualquer cliente consultar em tempo real todas as rela√ß√µes de entrada e sa√≠da de um ato normativo espec√≠fico, tornando o grafo acess√≠vel para outras aplica√ß√µes.
    *   **Visualiza√ß√£o:** A tarefa de visualiza√ß√£o √© atendida nativamente pelo **Neo4j Browser**, que j√° est√° dispon√≠vel no ambiente Docker e permite a execu√ß√£o de consultas Cypher para explorar o grafo de forma interativa.

**Conclus√£o da Tarefa:** O Bloco 4 est√° conclu√≠do. O projeto agora possui um grafo de KNOWLEDGE legislativo que n√£o apenas armazena a estrutura dos documentos, mas tamb√©m modela as complexas interconex√µes entre eles, com mecanismos de valida√ß√£o e uma API para consulta.

---

### Data: 2025-10-14

**Tarefa:** Teste da Camada de Recupera√ß√£o (Retriever) - INTERROMPIDO

**Detalhes:**

1.  **Objetivo:** Iniciar a valida√ß√£o pr√°tica da Tarefa 5.1, testando o script `src/retriever/main.py` com um plano de testes de 4 cen√°rios para avaliar as buscas lexical, sem√¢ntica e a l√≥gica de boost.
2.  **Incidente:** Na execu√ß√£o do primeiro teste (`python3 src/retriever/main.py --query "quais os direitos dos trabalhadores"`), o script falhou ao tentar conectar com o PostgreSQL, retornando o erro: `FATAL: could not open file "global/pg_filenode.map": Input/output error`.
3.  **Diagn√≥stico:** Este erro indica uma corrup√ß√£o severa no volume de dados do cont√™iner do PostgreSQL. A causa prov√°vel √© um desligamento incorreto do ambiente Docker em algum momento anterior. A situa√ß√£o escalou para um travamento completo do ambiente Docker, impedindo a execu√ß√£o de comandos como `docker-compose down`.
4.  **Ponto de Parada:** Estamos bloqueados e n√£o podemos prosseguir com os testes ou qualquer outra tarefa que dependa do banco de dados at√© que o ambiente Docker seja restaurado.

**Plano de Retomada:**

1.  **A√ß√£o do Usu√°rio:** O usu√°rio ir√° reiniciar o ambiente Docker para resolver o travamento.
2.  **Verifica√ß√£o:** Ao retomar, o primeiro passo ser√° verificar a responsividade do Docker com o comando `docker ps -a`.
3.  **Reset do Ambiente:** Executar `docker-compose down -v` para garantir que o ambiente antigo e os volumes corrompidos sejam completamente removidos.
4.  **Recria√ß√£o do Ambiente:** Executar `docker-compose up -d` para iniciar uma nova inst√¢ncia limpa dos servi√ßos (Postgres, Neo4j, MinIO).
5.  **Repopula√ß√£o dos Dados:** Executar o pipeline de ingest√£o completo (ex: `python3 run_pipeline.py`) para carregar todos os dados nos bancos de dados rec√©m-criados.
6.  **Retomada dos Testes:** Ap√≥s a repopula√ß√£o, retornaremos ao **Passo 2: Execu√ß√£o dos Testes** do plano de teste do `retriever`, come√ßando novamente pelo Teste 1.

---

### Data: 2025-10-14

**Tarefa:** Migra√ß√£o da Infraestrutura Local para Supabase

**Detalhes:**

1.  **Decis√£o Arquitetural:** Para resolver problemas de recursos (espa√ßo em disco e mem√≥ria) no ambiente de desenvolvimento local e alinhar a arquitetura com um modelo de produ√ß√£o, foi decidido migrar a infraestrutura de dados do Docker para a nuvem do Supabase.
2.  **Escopo da Migra√ß√£o:**
    *   O cont√™iner local do **PostgreSQL** ser√° substitu√≠do pelo servi√ßo **Supabase Database**.
    *   O cont√™iner local do **MinIO** ser√° substitu√≠do pelo servi√ßo **Supabase Storage**.
    *   O cont√™iner do **Neo4j** ser√° mantido localmente.
3.  **Implementa√ß√£o:**
    *   O PRD, o `docker-compose.yml` e as tarefas foram atualizados.
    *   As credenciais foram configuradas no arquivo `.env`.
    *   Todos os m√≥dulos da aplica√ß√£o foram refatorados para usar conex√µes centralizadas (`src/utils/db.py` e `src/utils/storage.py`).
    *   O schema do banco de dados foi aplicado com sucesso na inst√¢ncia do Supabase.
4.  **Valida√ß√£o:** Ap√≥s um intenso processo de depura√ß√£o envolvendo conectividade (IPv4/IPv6), permiss√µes de banco de dados e nomes de pacotes, o fluxo de ponta a ponta foi validado: o `crawler` populou o banco e enviou arquivos para o Storage, e o `run_pipeline.py` baixou os arquivos e os processou com sucesso.

**Conclus√£o da Tarefa:** A migra√ß√£o para o Supabase foi conclu√≠da com sucesso. O ambiente de desenvolvimento est√° mais leve e alinhado com a produ√ß√£o.

---

### Data: 2025-10-14

**Tarefa:** Corre√ß√£o e Aprimoramento do M√≥dulo Parser

**Detalhes:**

1.  **Identifica√ß√£o do Problema:** Durante os testes de ingest√£o, foi identificado que o parser estava extraindo "lixo" (cabe√ßalhos, rodap√©s, etc.) dos PDFs do Di√°rio Oficial de Goi√°s, corrompendo os dados dos dispositivos.
2.  **Decis√£o Arquitetural:** Para resolver o problema de forma robusta e escal√°vel, foram tomadas duas decis√µes:
    *   **Utilizar Texto Limpo da API:** Em vez de processar o PDF, o parser para fontes de Goi√°s foi modificado para usar o campo `conteudo_sem_formatacao` dispon√≠vel nos metadados da API, que j√° fornece o texto limpo do ato normativo.
    *   **Implementar Estrat√©gias de Parsing:** O parser foi refatorado para uma arquitetura de "estrat√©gias", onde cada tipo de fonte (ex: JSON da API de Goi√°s, HTML do Planalto) ter√° seu pr√≥prio m√≥dulo de parsing. O `parser/main.py` agora atua como um orquestrador que seleciona a estrat√©gia correta com base na URN da fonte.
3.  **Implementa√ß√£o e Depura√ß√£o:**
    *   Foi criada a estrat√©gia `src/parser/strategies/goias_api_json.py`.
    *   O orquestrador `src/parser/main.py` foi implementado.
    *   O `run_pipeline.py` foi ajustado para passar os metadados completos para o parser.
    *   Ap√≥s uma s√©rie de depura√ß√µes envolvendo erros de regex, `NameError` e `NotNullViolation` (devido a manifestos incompletos e dados parciais de execu√ß√µes anteriores), o `loader` tamb√©m foi tornado idempotente para o Neo4j.
4.  **Valida√ß√£o:** O pipeline completo foi executado com sucesso para uma fonte de teste, confirmando que a nova arquitetura do parser funciona, os dados s√£o extra√≠dos de forma limpa e carregados corretamente em ambos os bancos de dados.

**Conclus√£o da Tarefa:** O parser foi significativamente aprimorado, tornando-se mais robusto, preciso e extens√≠vel. O problema de extra√ß√£o de ru√≠do foi resolvido.

---

### Data: 2025-10-14

**Tarefa:** Substitui√ß√£o do Parser H√≠brido por Modelo de IA (LLM)

**Detalhes:**

1.  **Decis√£o Arquitetural:** Ap√≥s uma an√°lise aprofundada sobre a complexidade e a fragilidade da abordagem de parsing baseada em regex para lidar com as varia√ß√µes de formato dos documentos legais, foi decidido substituir a l√≥gica de extra√ß√£o de dispositivos por um modelo de linguagem grande (LLM).
2.  **Nova Abordagem:** Em vez de usar regex para encontrar artigos, par√°grafos, etc., o sistema agora far√° uma chamada a uma API de LLM externa. O modelo ser√° instru√≠do, via prompt, a receber o texto bruto de um ato normativo e a retornar diretamente a estrutura JSON completa e aninhada dos dispositivos, j√° no formato esperado pelo `loader`.
3.  **Vantagens:**
    *   **Maior Robustez:** A capacidade do LLM de entender o contexto sem√¢ntico torna o parsing muito mais resiliente a varia√ß√µes de formata√ß√£o.
    *   **Simplifica√ß√£o do C√≥digo:** A complexa l√≥gica de `parse_recursivo_texto` e as m√∫ltiplas express√µes regulares ser√£o substitu√≠das por uma √∫nica chamada de API e um prompt bem definido.
    *   **Escalabilidade:** A adapta√ß√£o para novos tipos de documentos se torna uma tarefa de engenharia de prompt, em vez de desenvolvimento de novas regex.
4.  **Plano de Implementa√ß√£o:**
    *   Ser√° criado um novo m√≥dulo `src/preprocessor/` para encapsular a l√≥gica de chamada √† API do LLM.
    *   A fun√ß√£o `parse` nas estrat√©gias de parsing (como `goias_api_json.py`) ser√° modificada para chamar o `preprocessor` em vez da l√≥gica de regex.
    *   O sistema est√° sendo preparado para integrar a API de LLM que ser√° escolhida pelo usu√°rio.

**Conclus√£o da Tarefa:** A decis√£o foi tomada e documentada. Esta mudan√ßa representa um salto significativo na robustez e na capacidade de generaliza√ß√£o do pipeline de ingest√£o.
---

### Data: 2025-10-14

**Tarefa:** Redefini√ß√£o e Refatora√ß√£o do Pipeline de Ingest√£o

**Detalhes:**

1.  **Decis√£o Arquitetural:** Foi definido um novo fluxo de trabalho para o pipeline de ingest√£o, com etapas mais claras e responsabilidades bem definidas para cada m√≥dulo, visando maior robustez e manutenibilidade.

2.  **Novo Fluxo do Pipeline:**
    *   **1. Crawler:** Respons√°vel por executar *spiders* espec√≠ficos de cada fonte (ex: Casa Civil de Goi√°s). O objetivo √© descobrir novos atos normativos e popular a tabela `fonte_documento` com metadados iniciais e o **`status = 'PENDENTE'`**.
    *   **2. Run Pipeline (Orquestrador):** O processo agora √© iniciado para todas as fontes com **`status = 'PENDENTE'`** ou **`'FALHA'`**.
    *   **3. Armazenamento do Documento Bruto:** O primeiro passo do orquestrador para cada fonte √© obter o documento oficial bruto (ex: o PDF do Di√°rio Oficial) e salv√°-lo no Supabase Storage para fins de auditoria e registro.
    *   **4. Extra√ß√£o de Texto:** Uma estrat√©gia de extra√ß√£o de texto espec√≠fica para a fonte √© acionada. **Para Goi√°s, o texto √© extra√≠do do campo `conteudo_sem_formatacao` dos metadados da API**, e n√£o do PDF armazenado.
    *   **5. Parser (LLM "One-Shot"):** O texto extra√≠do √© enviado para o `preprocessor`, que usa um LLM (Gemini) para converter o texto bruto diretamente em um JSON estruturado e hier√°rquico.
    *   **6. Normalizer:** O JSON da IA passa por uma etapa de valida√ß√£o e normaliza√ß√£o para garantir a compatibilidade com o schema.
    *   **7. Loader:** O JSON normalizado √© carregado no PostgreSQL (Supabase) e no Neo4j.
    *   **8. Enriquecimento e Valida√ß√£o:** Os m√≥dulos `linker` e `temporal_validator` s√£o executados para criar rela√ß√µes jur√≠dicas e checar a consist√™ncia temporal.
    *   **9. Finaliza√ß√£o:** Se todas as etapas forem conclu√≠das com sucesso, o status da fonte √© atualizado para **`'PROCESSADO'`**.

**Conclus√£o da Tarefa:** A nova arquitetura do pipeline foi documentada e servir√° como guia para as refatora√ß√µes de c√≥digo a seguir.
---

### Data: 2025-10-14

**Tarefa:** Depura√ß√£o e Valida√ß√£o Final do Pipeline de Ingest√£o

**Detalhes:**

1.  **Diagn√≥stico do Travamento:** Ap√≥s a refatora√ß√£o, o pipeline come√ßou a travar na etapa do parser. A investiga√ß√£o inicial apontou para a chamada √† API do Gemini, mas a falta de logs detalhados impedia a confirma√ß√£o.
2.  **Instrumenta√ß√£o e Depura√ß√£o:** Foram adicionados logs detalhados em todos os m√≥dulos (`run_pipeline`, `parser`, `preprocessor`, `linker`) e a flag `-u` (unbuffered) foi usada para for√ßar a exibi√ß√£o de logs de subprocessos em tempo real.
3.  **Identifica√ß√£o de Erros em Cascata:** A instrumenta√ß√£o revelou uma s√©rie de erros de programa√ß√£o (`NameError` e `ImportError`) nos m√≥dulos `parser` e `linker`, que foram introduzidos durante as refatora√ß√µes e estavam causando as falhas silenciosas.
4.  **Corre√ß√£o e Valida√ß√£o em Lote:** Todos os bugs identificados foram corrigidos. Em seguida, um lote de 10 documentos foi processado com sucesso, com o pipeline executando todas as etapas (Parser, Normalizer, Loader, Linker, etc.) sem erros. O tempo de processamento do parser por documento variou entre 20 e 90 segundos, confirmando que a abordagem "one-shot" com LLM √© vi√°vel.
5.  **Isolamento do Problema Original:** Foi confirmado que o travamento original √© provavelmente espec√≠fico do documento `decreto.numerado;2025-01-31;10634`, cujo tamanho ou conte√∫do pode estar causando um problema na API externa. O pipeline em si foi provado robusto.

**Conclus√£o da Tarefa:** O pipeline de ingest√£o foi completamente depurado, validado e est√° est√°vel para processamento em lote. A arquitetura refatorada est√° funcional.
---

### Data: 2025-10-14

**Tarefa:** Refatora√ß√£o do Parser para a Arquitetura "Segmentar e Conquistar"

**Detalhes:**

1.  **Diagn√≥stico:** A abordagem de parsing "one-shot" com LLM, embora funcional para documentos menores, falhou consistentemente com um erro de `504 Deadline Exceeded` em documentos grandes (ex: `decreto.numerado;2025-01-31;10634`), provando-se n√£o ser robusta para todos os casos.

2.  **Nova Decis√£o Arquitetural:** Foi implementada uma arquitetura h√≠brida mais inteligente, chamada "Segmentar e Conquistar", para lidar com documentos de qualquer tamanho de forma mais resiliente.

3.  **Novo Fluxo do Parser:**
    *   **Etapa 1 (Segmentar com LLM):** Uma chamada r√°pida √† API do Gemini (`ai_clean_text`) √© usada para a tarefa de alto n√≠vel de separar o texto bruto em `corpo_legislativo` e `anexos`. Esta abordagem substitui a regex que se mostrou ineficaz para esta tarefa.
    *   **Etapa 2 (Dividir com Regex):** O `corpo_legislativo` resultante √© ent√£o dividido em uma lista de "chunks" (um para cada artigo) usando uma regex simples e confi√°vel que busca por `Art. ...`.
    *   **Etapa 3 (Conquistar com LLM):** O sistema itera sobre a lista de artigos. Para cada artigo, uma chamada focada √† API do Gemini (`ai_parse_article_structure`) √© feita, solicitando a estrutura√ß√£o hier√°rquica (par√°grafos, incisos, etc.) apenas daquele pequeno trecho de texto.

4.  **Vantagens:** Esta abordagem modular evita timeouts ao reduzir drasticamente a carga de trabalho de cada chamada √† IA, aumenta a confiabilidade ao dar ao modelo tarefas mais simples e focadas, e melhora a capacidade de depura√ß√£o, isolando falhas a um artigo espec√≠fico em vez do documento inteiro.

**Conclus√£o da Tarefa:** A nova arquitetura foi documentada e est√° sendo implementada para criar um pipeline de parsing mais robusto e escal√°vel.
---

### Data: 2025-10-15

**Tarefa:** Implementa√ß√£o da Arquitetura de Parser "Segmentar e Conquistar"

**Detalhes:**

1.  **Implementa√ß√£o da Arquitetura:** Para resolver os problemas de timeout com documentos grandes, a arquitetura do parser foi completamente refatorada para o modelo "Segmentar e Conquistar".
2.  **Cria√ß√£o de M√≥dulos Core:** Foram criados m√≥dulos determin√≠sticos e focados: `schemas.py` (para contratos de I/O), `core/normalizer.py` (limpeza de texto), `core/segmenter.py` (fallback de anexo com regex), `core/article_splitter.py` (divis√£o por artigos) e `core/article_parser_fsm.py` (m√°quina de estados para a estrutura interna do artigo).
3.  **Refatora√ß√£o do Preprocessor:** O m√≥dulo `preprocessor` foi aprimorado com a fun√ß√£o `ai_segment_document`, que implementa uma busca inteligente por janela deslizante para encontrar o fim do corpo legal, substituindo a abordagem "one-shot".
4.  **Orquestra√ß√£o:** O `parser/main.py` foi reescrito para orquestrar todas essas etapas, e o `run_pipeline.py` foi ajustado para invoc√°-lo corretamente.

**Conclus√£o da Tarefa:** A nova arquitetura foi totalmente implementada, tornando o parser mais robusto, modular e resiliente.

---

### Data: 2025-10-15

**Tarefa:** Depura√ß√£o de Infraestrutura (Docker/Neo4j) e API (Gemini)

**Detalhes:**

Durante os testes da nova arquitetura, uma s√©rie de erros em cascata foi identificada e resolvida.

1.  **Erro de Conex√£o com Neo4j:** O erro inicial `Connection refused` foi diagnosticado como o daemon do Docker n√£o estar em execu√ß√£o. Ap√≥s a inicializa√ß√£o, o erro mudou para `Connection reset by peer`. A causa raiz foi identificada como uma incompatibilidade de TLS, resolvida adicionando o par√¢metro `encrypted=False` a todas as chamadas do driver Neo4j nos m√≥dulos `loader`, `linker` e `temporal_validator`.
2.  **Erro de API do Gemini:** As chamadas √† IA falhavam com um erro `404 Not Found`. Seguindo a sugest√£o da pr√≥pria mensagem de erro, foi criado e executado um script tempor√°rio (`list_available_models.py`) que listou os modelos corretos dispon√≠veis para a chave de API. A lista de modelos no `preprocessor` foi atualizada com os nomes corretos (`models/gemini-2.5-flash`, `models/gemini-2.5-pro`).

**Conclus√£o da Tarefa:** Os problemas de infraestrutura e configura√ß√£o de API foram completamente resolvidos, preparando o ambiente para uma valida√ß√£o completa.

---

### Data: 2025-10-15

**Tarefa:** Valida√ß√£o Final do Pipeline de Ingest√£o Refatorado

**Detalhes:**

1.  **Execu√ß√£o do Teste:** O pipeline foi executado novamente com a URN que historicamente causava timeouts (`decreto.numerado;2025-01-31;10634`).
2.  **Sucesso do Parser:** A etapa de segmenta√ß√£o com IA funcionou corretamente, identificando o fim do corpo legal sem timeouts e validando a nova arquitetura.
3.  **Sucesso do Grafo:** As etapas de `Loader`, `Linker` e `Temporal Validator` se conectaram com sucesso ao Neo4j e executaram suas l√≥gicas.
4.  **Conclus√£o do Pipeline:** O documento foi processado de ponta a ponta, e seu status no banco de dados foi atualizado para `PROCESSADO`.

**Conclus√£o da Tarefa:** A refatora√ß√£o foi um sucesso. O pipeline est√° agora robusto e capaz de processar documentos grandes e complexos. Foram identificados pontos de melhoria secund√°rios no `Linker` (que n√£o encontrou textos para vincular) e no `Temporal Validator` (que reportou inconsist√™ncias), a serem investigados a seguir.
