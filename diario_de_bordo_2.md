# üö¢ Di√°rio de Bordo ‚Äì Projeto Atlas (Parte 2)

Este documento continua o registro de tarefas ap√≥s a ocorr√™ncia de erros de API no arquivo original.

---

### Data: 2025-10-13

**Tarefa:** 3.4 - Implementar normalizador

**Detalhes:**

1.  **Cria√ß√£o do Normalizador:** Foi criado o diret√≥rio `normalizer/` e o script `normalizer.py`, que aceita como entrada um arquivo JSON gerado pelo parser.
2.  **Implementa√ß√£o da Regra #1:** O script foi implementado para percorrer a √°rvore de dispositivos e remover refer√™ncias legislativas entre par√™nteses (ex: `(Vide Lei...)`, `(Reda√ß√£o dada por...)`).
3.  **Auditoria:** A l√≥gica para preservar o texto original (`texto_original_parser`), adicionar um bloco de auditoria (`normalizacao`) e gerar um novo hash para o texto limpo (`hash_texto_normalizado`) foi implementada.
4.  **Execu√ß√£o e Gera√ß√£o:** O normalizador foi executado com sucesso para os tr√™s atos normativos do escopo (CF, CE-GO, LO-AP), gerando os respectivos arquivos `_normalizado.json`, `_diff.md` e logs.

**Conclus√£o da Tarefa:** A tarefa 3.4 est√° conclu√≠da. Temos um normalizador funcional com a primeira regra de limpeza implementada e validada para todo o nosso corpus.

---

### Data: 2025-10-13

**Tarefa:** 3.5 - Criar pipeline de carga (ETL)

**Detalhes:**

1.  **Ambiente do Loader:** Foi criado o diret√≥rio `loader/` e o script `loader.py`, com as depend√™ncias para PostgreSQL e Neo4j.
2.  **Corre√ß√£o de Schema (Postgres):** O schema do PostgreSQL foi corrigido para adicionar as colunas `parent_id` (em `dispositivo`) e `texto_original` (em `versao_textual`), e para renomear `hash_texto` para `hash_texto_normalizado`, alinhando o banco com os dados gerados.
3.  **Desenvolvimento do Loader:** O script `loader.py` foi implementado para ler um arquivo JSON normalizado e carregar os dados de forma hier√°rquica em ambos os bancos de dados.
4.  **Execu√ß√£o e Valida√ß√£o:** O loader foi executado com sucesso para a Lei Org√¢nica de Aparecida. A valida√ß√£o confirmou que os dados foram inseridos corretamente no PostgreSQL (incluindo a rela√ß√£o `parent_id`) e no Neo4j (com a cria√ß√£o de n√≥s e relacionamentos `[:CONTEM]`).

**Conclus√£o da Tarefa:** A tarefa 3.5 est√° conclu√≠da. Temos um pipeline de ETL funcional que popula nossos bancos de dados a partir dos arquivos JSON processados.

---

### Data: 2025-10-13

**Tarefa:** 3.6 - Criar rotina de verifica√ß√£o de integridade

**Detalhes:**

1.  **Cria√ß√£o do Verificador:** Foi criado o diret√≥rio `integrity_checker/` com o script `checker.py`. O objetivo do script √© comparar os dados dos arquivos `_normalizado.json` (a "verdade") com o que foi carregado no PostgreSQL e no Neo4j.

2.  **Processo de Depura√ß√£o Intenso:**
    *   Uma `SyntaxError` inicial no script, causada por uma f-string mal formatada, levou a m√∫ltiplas tentativas de corre√ß√£o. O problema se mostrou complexo de depurar, exigindo a cria√ß√£o de scripts de teste isolados.
    *   Ap√≥s a corre√ß√£o da sintaxe, o script executou, mas falhou em se conectar ao Neo4j devido a credenciais incorretas. O arquivo `.env` foi corrigido para usar a senha `atlas_password`, alinhando-o com o `docker-compose.yml`.
    *   Com a conex√£o bem-sucedida, o script passou a falhar na consulta de hashes no PostgreSQL e reportar um aviso no Neo4j. A an√°lise dos schemas e do `loader.py` revelou que os nomes das propriedades dos hashes estavam incorretos nas consultas do checker. As queries foram corrigidas para usar `hash_texto_normalizado` no PG e `hash` no Neo4j.
    *   Finalmente, a extra√ß√£o dos dados do JSON ("ground truth") estava falhando, retornando 0 dispositivos. Uma an√°lise detalhada da estrutura do JSON de sa√≠da do normalizador revelou que a chave correta era `dispositivos` (plural) no n√≠vel raiz, e n√£o `dispositivo` aninhado em `ato_normativo`.

3.  **Execu√ß√£o e Descoberta Cr√≠tica:**
    *   Ap√≥s todas as corre√ß√µes, o `checker.py` foi executado para os tr√™s atos normativos.
    *   **Resultado:** O script executou com sucesso, mas reportou **FALHA** na integridade dos dados para todos os atos carregados ap√≥s o primeiro.
    *   **Diagn√≥stico:** A leitura dos relat√≥rios gerados (`*_integrity_report.md`) mostrou que os dados no PostgreSQL estavam **corretos e √≠ntegros**. No entanto, o banco de dados Neo4j estava **corrompido**.
    *   **Causa Raiz:** O `loader.py` n√£o isola os dispositivos por ato no Neo4j. Ele cria n√≥s de `Dispositivo` usando um `caminho_estrutural` (ex: `art-1`) que n√£o √© globalmente √∫nico. Ao carregar m√∫ltiplos atos, ele cria rela√ß√µes `[:CONTEM]` amb√≠guas, conectando dispositivos de um ato ao grafo de outro. O checker encontrou `170` dispositivos no grafo da Lei Org√¢nica, que na verdade √© o total de dispositivos de todos os atos carregados.

**Conclus√£o da Tarefa:** A tarefa 3.6 est√° **conclu√≠da**. O script `integrity_checker/checker.py` foi finalizado e se provou uma ferramenta de QA **essencial**, ao identificar um bug cr√≠tico de corrup√ß√£o de dados no `loader.py` (tarefa 3.5) que n√£o havia sido pego anteriormente. Os relat√≥rios gerados documentam a falha com precis√£o. A corre√ß√£o do loader ser√° a pr√≥xima prioridade.

---

### Data: 2025-10-13

**Tarefa:** Corre√ß√£o do Bug do Loader e Teste de Pipeline Completo

**Detalhes:**

1.  **Corre√ß√£o do `loader.py`:**
    *   A fun√ß√£o `insert_neo4j` foi reescrita para usar um identificador globalmente √∫nico (`id_unico`) para cada n√≥ `Dispositivo`, formado pela `urn_lexml` do ato e pelo `caminho_estrutural` do dispositivo.
    *   Todas as cl√°usulas `MATCH` no `loader` foram atualizadas para usar este novo `id_unico`, garantindo que os relacionamentos `[:CONTEM]` sejam criados apenas dentro do escopo do ato normativo correto.
    *   A fun√ß√£o `insert_postgres` tamb√©m foi validada para garantir que lia a estrutura de dados JSON corretamente.

2.  **Limpeza dos Bancos de Dados:**
    *   Os bancos de dados, que estavam em um estado corrompido, foram completamente limpos para garantir um teste v√°lido.
    *   **PostgreSQL:** O comando `TRUNCATE TABLE ... CASCADE` foi executado.
    *   **Neo4j:** O comando `MATCH (n) DETACH DELETE n` foi executado.

3.  **Teste de Pipeline End-to-End:**
    *   O `loader.py` corrigido foi executado sequencialmente para os tr√™s atos normativos (LO-AP, CE-GO, CF).
    *   O `integrity_checker.py` foi executado para cada um dos tr√™s atos ap√≥s o carregamento.

4.  **Valida√ß√£o Final:**
    *   **Resultado:** Todos os tr√™s testes de integridade passaram com **sucesso**.
    *   A corre√ß√£o foi **validada**, e o pipeline de dados (Parser ‚Üí Normalizer ‚Üí Loader ‚Üí Checker) agora est√° robusto e livre de corrup√ß√£o de dados entre os atos.

**Conclus√£o:** O bug cr√≠tico do loader foi corrigido e todo o pipeline de ingest√£o foi validado com sucesso. O projeto est√° novamente em um estado est√°vel e pronto para as pr√≥ximas etapas.

---

### Data: 2025-10-13

**Tarefa:** Teste de Estresse do Pipeline com Novas Leis e Corre√ß√£o do Parser HTML

**Detalhes:**

1.  **Objetivo:** Validar a robustez do pipeline com novas leis e, ao mesmo tempo, implementar a funcionalidade de parsing de HTML que estava ausente.

2.  **Processo de Refatora√ß√£o do Parser:**
    *   O `parser/main.py` foi profundamente refatorado para remover a depend√™ncia de metadados "hardcoded" (`METADATA_OVERRIDES`).
    *   Foi introduzida uma interface de linha de comando (CLI) que opera com arquivos de **manifesto** (`.json`), tornando o parser configur√°vel e desacoplado.
    *   A l√≥gica de parsing de PDF foi corrigida e validada com uma nova lei municipal, que foi processada com sucesso de ponta a ponta.
    *   A fun√ß√£o `parse_html` foi implementada com `BeautifulSoup` para extrair artigos de documentos HTML de forma gen√©rica.

3.  **Corre√ß√£o de Bug no Loader (PostgreSQL):**
    *   Durante a ingest√£o da LAI (HTML), um erro de `value too long` ocorreu no PostgreSQL.
    *   O `loader.py` foi depurado, identificando que a coluna `rotulo` na tabela `dispositivo` era `VARCHAR(255)`, insuficiente para alguns textos de artigos.
    *   O schema do PostgreSQL foi alterado (`ALTER TABLE`), mudando a coluna `rotulo` para o tipo `TEXT`.

4.  **Valida√ß√£o Final do Pipeline HTML:**
    *   Ap√≥s a corre√ß√£o do schema e a limpeza dos bancos de dados, o pipeline completo foi re-executado para a LAI.
    *   **Resultado:** A Lei de Acesso √† Informa√ß√£o (HTML) foi processada com **sucesso**, com o `integrity_checker` confirmando a integridade dos dados.

**Conclus√£o:** O teste de estresse foi conclu√≠do com sucesso. O pipeline agora √© capaz de processar tanto PDFs quanto HTMLs de fontes externas. A refatora√ß√£o do parser e a corre√ß√£o do schema do banco de dados tornaram o sistema significativamente mais robusto e flex√≠vel. O projeto est√° pronto para avan√ßar para a pr√≥xima tarefa do plano original.

---

### Data: 2025-10-13

**Tarefa:** Teste de Integra√ß√£o Final do Pipeline Completo

**Detalhes:**

1.  **Objetivo:** Realizar um teste de ponta a ponta com todo o corpus legislativo coletado at√© o momento para garantir a estabilidade e robustez do pipeline ap√≥s as m√∫ltiplas refatora√ß√µes e corre√ß√µes de bugs.

2.  **Prepara√ß√£o do Ambiente:**
    *   **Limpeza de Artefatos:** Todos os arquivos gerados anteriormente nos diret√≥rios `parser/output`, `normalizer/output` e `integrity_checker/output` foram removidos.
    *   **Limpeza dos Bancos de Dados:** Os bancos de dados PostgreSQL e Neo4j foram completamente zerados para garantir que n√£o houvesse dados residuais de execu√ß√µes anteriores.

3.  **Execu√ß√£o do Pipeline:**
    *   O pipeline completo (Parser ‚Üí Normalizer ‚Üí Loader ‚Üí Checker) foi executado em sequ√™ncia para os seguintes 5 atos normativos, processando os textos na √≠ntegra:
        1.  Constitui√ß√£o Federal (HTML)
        2.  Constitui√ß√£o de Goi√°s (HTML)
        3.  Lei Org√¢nica de Aparecida de Goi√¢nia (PDF)
        4.  Lei de Acesso √† Informa√ß√£o (HTML)
        5.  Lei Municipal N¬∫ 2.229/2001 de Aparecida de Goi√¢nia (PDF)

4.  **Resultado:**
    *   **SUCESSO:** Todos os cinco atos foram processados com sucesso, e o `integrity_checker` validou a integridade dos dados para cada um deles, confirmando que a carga no PostgreSQL e no Neo4j foi correta e sem corrup√ß√£o de dados.

**Conclus√£o:** O teste de integra√ß√£o final foi um sucesso. O pipeline de ingest√£o est√° est√°vel e validado para o corpus atual. O Bloco 3 do plano de tarefas est√° significativamente avan√ßado. O projeto est√° pronto para prosseguir para a pr√≥xima etapa: **3.7 - Gerar embeddings por dispositivo**.